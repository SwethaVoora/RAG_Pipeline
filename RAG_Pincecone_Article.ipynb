{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15a6e10b",
   "metadata": {},
   "source": [
    "# Installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a6c47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU langchain_community pypdf langchain-openai langchain-text-splitters langgraph langchain  faiss-cpu pinecone python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d425f8c",
   "metadata": {},
   "source": [
    "# Setting Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc4b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import dotenv_values\n",
    "\n",
    "# env_vars = dotenv_values(\".env\")  # Loads all variables from .env\n",
    "# print(env_vars)  # This should print a dictionary with your environment variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3762c3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# # Load environment variables from .env file\n",
    "# load_dotenv(find_dotenv(\".env\"))\n",
    "\n",
    "# # Access the environment variables safely\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# # Ensure API keys are loaded correctly\n",
    "# if not OPENAI_API_KEY or not PINECONE_API_KEY:\n",
    "#     raise ValueError(\"Missing API Key(s)! Check your .env file.\")\n",
    "\n",
    "# print(\"Environment variables loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14a0b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "# os.environ[\"LANGSMITH_PROJECT\"] = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "# os.environ[\"LANGSMITH_TRACING_V2\"] = \"true\"\n",
    "# os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "\n",
    "# print(os.environ[\"PINECONE_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e7fa3e3-1eae-4b7c-8560-b79414fe5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "# Load API Keys Safely\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "LANGSMITH_PROJECT = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "LANGSMITH_TRACING_V2 = os.getenv(\"LANGSMITH_TRACING_V2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0eafbc",
   "metadata": {},
   "source": [
    "# Langsmith Tracing enabling\n",
    "- Why use Langmsith?\n",
    "    - Because, We want to see the logs and workings of every step of our RAG.\n",
    "    - It is used to implement observability and monitoring for our LLM Models, when they are invoked.\n",
    "    - How does tracing_is_enabled work?Does it always set tracing to true?or is it linked to LANGSMITH_TRACING_V2?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f820946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import utils\n",
    "utils.tracing_is_enabled()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8a7567",
   "metadata": {},
   "source": [
    "# **INDEXING THE DOCUMENTS(STATIC INDEXING):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6694516",
   "metadata": {},
   "source": [
    "# **INITIALIZING PINECONE CLIENT & LOGGING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97d2a2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/Users/swethavoora/Desktop/AI Projects/RAG_Pipeline/myenv/lib/python3.13/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:pinecone_plugin_interface.logging:Installing plugin inference into Pinecone\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableSequence\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Index Name\n",
    "index_name = \"testprojectv1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a91a56",
   "metadata": {},
   "source": [
    "# **Function to check if index exists, if not, create it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d400041",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ensure_index():\n",
    "    existing_indexes = [index[\"name\"] for index in pc.list_indexes()]\n",
    "    if index_name in existing_indexes:\n",
    "        logging.info(f\"Index '{index_name}' already exists. Skipping creation.\")\n",
    "    else:\n",
    "        logging.info(f\"Creating Pinecone index: {index_name}\")\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "        )\n",
    "        time.sleep(5)  # Ensure index is ready\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9c84e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/Users/swethavoora/Desktop/AI Projects/RAG_Pipeline/myenv/lib/python3.13/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1536,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'ns1': {'vector_count': 49}, 'ns2': {'vector_count': 49}},\n",
      " 'total_vector_count': 98}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = pc.Index(index_name)\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2bcea41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.delete(namespace='conv_vector_cda9c6d0-19ab-4e01-af89-10e7e7ba297d', delete_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54531846",
   "metadata": {},
   "source": [
    "# **Function to load and split documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b3f519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_split_documents(filepath):\n",
    "    logging.info(\"Loading document...\")\n",
    "    loader = PyPDFLoader(filepath)\n",
    "    docs = loader.load()\n",
    "\n",
    "    logging.info(\"Splitting documents into chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    return {\"all_splits\": splits, \"total_Splits:\": len(splits), \"message\": \"Documents loaded and split successfully!\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eb55cb",
   "metadata": {},
   "source": [
    "# **Function to embed documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79782438",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_documents(inputs):\n",
    "    splits = inputs[\"all_splits\"]\n",
    "    logging.info(\"Generating embeddings...\")\n",
    "    embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=OPENAI_API_KEY)\n",
    "    embeddings = embeddings_model.embed_documents([split.page_content for split in splits])\n",
    "\n",
    "    # Compute norms (to ensure embeddings aren't garbage)\n",
    "    norms = [sum(e[:5]) for e in embeddings[:5]]  # Get first 5 embeddings' norm\n",
    "    \n",
    "    return {\"all_embeddings\": embeddings, \"norms\": norms, \"message\": \"Embeddings generated successfully!\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc89166",
   "metadata": {},
   "source": [
    "# **Function to upsert embeddings into Pinecone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f35c422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upsert_embeddings(data):  # This function expects a dictionary\n",
    "    splits = data[\"splits\"][\"all_splits\"]\n",
    "    embeddings = data[\"embeddings\"][\"all_embeddings\"]\n",
    "    logging.info(f\"Upserting {len(embeddings)} documents into Pinecone...\")\n",
    "    index = pc.Index(index_name)\n",
    "\n",
    "    vectors = [\n",
    "        {\n",
    "            \"id\": f\"doc_{split.metadata.get('source')}_{i}_{split.metadata.get('page_label', 'no_label')}\",  # Ensure ID is valid and unique(because vectors of same Id get overwritten by the latest vector)\n",
    "            \"values\": emb,\n",
    "            \"metadata\": {\"text\": split.page_content}\n",
    "        }\n",
    "        for i, (split, emb) in enumerate(zip(splits, embeddings)) if len(emb) > 0\n",
    "    ]\n",
    "\n",
    "    BATCH_SIZE = 100  # Recommended batch size\n",
    "    for i in range(0, len(vectors), BATCH_SIZE):\n",
    "        batch = vectors[i:i + BATCH_SIZE]\n",
    "        index.upsert(vectors=batch, namespace='ns1')\n",
    "        logging.info(f\"Upserted batch {i // BATCH_SIZE + 1} of {len(vectors) // BATCH_SIZE + 1}\")\n",
    "    \n",
    "    logging.info(f\"Upserted {len(vectors)} vectors into the vector store.\")\n",
    "    return f\"Upserted {len(vectors)} vectors into the vector store.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979be44d",
   "metadata": {},
   "source": [
    "# **Using LangChain's RunnableSequence for an Indexing Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "015b0a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
    "\n",
    "# Turn Functions into Runnables\n",
    "load_split_runnable = RunnableLambda(load_and_split_documents)\n",
    "embed_runnable = RunnableLambda(embed_documents)\n",
    "upsert_runnable = RunnableLambda(upsert_embeddings)\n",
    "\n",
    "# Using LangChain's RunnableSequence to create the Indexing chain\n",
    "# indexing_chain = RunnableSequence(load_split_runnable, embed_runnable, upsert_runnable)\n",
    "\n",
    "\n",
    "# Using LangChain's | Operator for an Indexing Chain\n",
    "indexing_chain = (\n",
    "    load_split_runnable \n",
    "    | {\n",
    "        \"splits\": RunnablePassthrough(),\n",
    "        \"embeddings\": embed_runnable\n",
    "    }\n",
    "    | upsert_runnable\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5733c14a",
   "metadata": {},
   "source": [
    "# **Run the Indexing Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94d333f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_indexing_pipeline(filepath):\n",
    "    ensure_index()\n",
    "    \n",
    "    indexing_chain.invoke(filepath)\n",
    "    \n",
    "    logging.info(\"Indexing pipeline completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7a58f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Index 'testprojectv1' already exists. Skipping creation.\n",
      "INFO:root:Loading document...\n",
      "INFO:root:Splitting documents into chunks...\n",
      "INFO:root:Generating embeddings...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Upserting 49 documents into Pinecone...\n",
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/Users/swethavoora/Desktop/RAGProject/myenv/lib/python3.13/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n",
      "INFO:root:Upserted batch 1 of 1\n",
      "INFO:root:Upserted 49 vectors into the vector store.\n",
      "INFO:root:Indexing pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "run_indexing_pipeline(\"./data/BOFA_safedepositbox_disclosures.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69699025",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index('testprojectv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506ba24",
   "metadata": {},
   "source": [
    "# IGNORE THE BELOW CODE TILL RETRIEVAL & GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dd8a32",
   "metadata": {},
   "source": [
    "# Initializing a Pinecone Client\n",
    "- We are going to use Pinecone database for Indexing our documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11a2e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pinecone import Pinecone\n",
    "\n",
    "# pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d0b79d",
   "metadata": {},
   "source": [
    "# Creating an Index\n",
    "- What is an Index?\n",
    "  - An index defines the dimension of vectors to be stored and the similarity metric to be used when querying them.\n",
    "- Is it different from Vector store?\n",
    "- How does one know the dimension to be user?\n",
    "  - This can be known from the embedding model that you are willing to choose. Each embedding model provides vectors of fixed length. Meaning, when they convert the text into numerical representation, the numerical representation will be a series of numbers of this fixed length. \n",
    "  - Eg : 1536 for OpenAI's text-embedding-3-small model\n",
    "- We are going to use static Indexing, meaning, we are going to Index our curated set of documents only once(or periodically when we want to update our Index). All the end users will use the RAG, which will retrieve from the same Index.\n",
    "- Advantage with Pinecone is that it is serverless. So, we dont have to worry about ...?WWWWHHHHHAAAAAATTTTT???!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80fb4f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pinecone import ServerlessSpec\n",
    "# index_name = \"testprojectv1\"\n",
    "\n",
    "# pc.create_index(\n",
    "#     name=index_name,\n",
    "#     dimension=1536, # Replace with your model dimensions\n",
    "#     metric=\"cosine\", # Replace with your model metric\n",
    "#     spec=ServerlessSpec(\n",
    "#         cloud=\"aws\",\n",
    "#         region=\"us-east-1\"\n",
    "#     ) \n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d98bf1",
   "metadata": {},
   "source": [
    "# Document Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4c70a7",
   "metadata": {},
   "source": [
    "- document loaders return a list of document objects.\n",
    "- each page in the original document is instantiated as a document object with page content, metadata and other optional attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d60347",
   "metadata": {},
   "source": [
    "# Splitting the documents into chunks\n",
    "- because, all the models have limit for the context window(len(input + len(output)))\n",
    "- so, having these chunks will help us retrieve docs and stay within the approved context window length. instead of taking the entire document as the model's context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ad42cb",
   "metadata": {},
   "source": [
    "# Defining the Embeddings model, which can be used to embed chunks of text into numerical representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2590edcc",
   "metadata": {},
   "source": [
    "- Each numerical representation store the semantic meaning of the chunk of information. which can later be used for proper retrieval of relevant information based on similarity search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d615dea",
   "metadata": {},
   "source": [
    "# Creating a function to embed splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf910545",
   "metadata": {},
   "source": [
    "# Embedding the splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678ae96f",
   "metadata": {},
   "source": [
    "# Storing the embeddings in Pinecone\n",
    "- Upsert the six generated vector embeddings into a new ns1 namespace in your index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e964f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2440fba",
   "metadata": {},
   "source": [
    "# This marks the completion of Indexing part of the RAG pipeline\n",
    "- Choosing a source of information\n",
    "- Loading the source of information(the document)\n",
    "- Once loaded, we split the document into chunks to suit the context length for most of the llm models\n",
    "- Once split, we define our desired embedding model\n",
    "- Using the embedding model, we create our vector store using FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a6d661",
   "metadata": {},
   "source": [
    "# RETRIEVAL AND GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c2860b",
   "metadata": {},
   "source": [
    "### CREATING MODELS, PROMPTS AND CHAINS\n",
    "- These will be used as part of our retrieval and generation stages of a RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f7ad91",
   "metadata": {},
   "source": [
    "# CHAINING\n",
    "- To implement chain of retriever -> prompt -> model invocation -> output parser, we might need runnables.\n",
    "- What are runnables? These are the classes or functions which implement a invoke method to take an input, process it and then provide an output\n",
    "- All these runnables can be chained using the '|' pipe operator\n",
    "- This '|' operator will ensure that the output of one runnable becomes the input of th efollowing runnable in the chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f517b7d5",
   "metadata": {},
   "source": [
    "# Defining the Embedding Model(same as the one we used for Indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "641043ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pinecone_plugin_interface.logging:Discovering subpackages in _NamespacePath(['/Users/swethavoora/Desktop/RAGProject/myenv/lib/python3.13/site-packages/pinecone_plugins'])\n",
      "INFO:pinecone_plugin_interface.logging:Looking for plugins in pinecone_plugins.inference\n"
     ]
    }
   ],
   "source": [
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=OPENAI_API_KEY)\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a196a2",
   "metadata": {},
   "source": [
    "# **Creating the retriever**\n",
    "- This retriever will have the question parameter (It takes one input argument)\n",
    "- This retriever will retrieve the relevant chunks or docs from the vector store index using 'similarity search'.\n",
    "- We use include_metadata in the index.query, because our page_content(the text) is actually part of our metadata key.\n",
    "- We can always modify the way we want our documents to be.\n",
    "- Knowing the structure of the payload/documents helps us write correct code for the RAG chain\n",
    "- always include the question in []. because embed_documents expects a list. \n",
    "- If its not a list, then each character will be treated as a separate document that needs to be embedded seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a58093ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(question):\n",
    "    # always include the question in []. because embed_documents expects a list. \n",
    "    # If its not a list, then each character will be treated as a separate document that needs to be embedded seperately.\n",
    "    embeddedQuestion = embeddings_model.embed_documents([question]) \n",
    "    similar_docs = index.query(vector=embeddedQuestion, top_k=3, namespace=\"ns1\", include_metadata=True)\n",
    "    return similar_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ae0039",
   "metadata": {},
   "source": [
    "# **Modify the retrieved_docs to be a context of certain format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23f19cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatContext(retrieved_docs):\n",
    "    return \"\\n\".join(doc.metadata[\"text\"] for doc in retrieved_docs['matches'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0064e13",
   "metadata": {},
   "source": [
    "# Converting the retriever and formatcontext into runnables as '|' can only be used to chain runnables\n",
    "- Runnables are components or classes that implement the 'invoke' method inside them\n",
    "- Or, the regular functions need to be wrapped by the runnablelambda wrapper to convert them into runnables\n",
    "- OR WE COULD DIRECTLY USE THE FUNCTIONS ENCLOSED IN A LIST, AS AN ARGUMENT TO RUNNABLESEQUENCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f689f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Wrap retriever and formatContext as runnables\n",
    "retriever_runnable = RunnableLambda(retriever)  # Wrap retriever\n",
    "formatContext_runnable = RunnableLambda(formatContext)  # Wrap formatContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ab7e3",
   "metadata": {},
   "source": [
    "# **Prompt Templating**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f981382",
   "metadata": {},
   "source": [
    "- prompt templating is something we use to provide the model with instructions on how it should behave, what will it receive as part of the prompt.\n",
    "- in this case, the context is the formatted context created from the retrieved chunks from the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8aff7194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Answer the user question based on the following context.\n",
    "    If you dont know the answer, just say you dont know.\n",
    "                                          \n",
    "    Context: {context} \n",
    "                                          \n",
    "    Question: {question}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe895eac",
   "metadata": {},
   "source": [
    "# **Creating a gpt-3.5-turbo model**\n",
    "- This is the LLM model that will be invoked by the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93a84adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(openai_api_key = OPENAI_API_KEY, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43048a6f",
   "metadata": {},
   "source": [
    "# String Parser \n",
    "- To get only content as the output from the AIMessage object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ccd98d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputParser(response):\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed984e0",
   "metadata": {},
   "source": [
    "# USING RUNNABLE SEQUENCE TO CHAIN THE FUNCTIONS THEMSELVES(BELOW COMMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a711681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "# indexing_chain = RunnableSequence([load_and_split_documents, embed_documents, upsert_embeddings])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5ddada",
   "metadata": {},
   "source": [
    "# Defining the RAG-pipeline Chain\n",
    "- Outer pipeline:\n",
    "    - Inner pipeline (retriever | formatContext):\n",
    "        - retriever will fetch relevant docs\n",
    "        - formatContext will format the retrieved documents\n",
    "    - Dictionary of {context, question} are passed to prompt\n",
    "    - The output of prompmt is passed to the model\n",
    "    - The output of the model is passed to the outputParser\n",
    "    - The User receives the output of the entire chain (The content of the AIMessage object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d2266",
   "metadata": {},
   "source": [
    "when we create a chain like below: \n",
    "- the input (your question string) is passed individually to each key in that dictionary. In other words:\n",
    "\n",
    "-- “context” key:\n",
    "\n",
    "The retriever receives the same input string (“What happens if ketu is in leo?”).\n",
    "The retriever internally uses that string to embed and do a similarity search in FAISS.\n",
    "Whatever the retriever returns is piped to formatContext.\n",
    "\n",
    "-- “question” key:\n",
    "\n",
    "RunnablePassthrough() also receives the same input string. It simply passes it along with no change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "883971ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever_runnable | formatContext_runnable, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | outputParser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7840d6",
   "metadata": {},
   "source": [
    "# Invoking the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7dd024e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'At Bank of America (BOFA), renters are prohibited from placing additional locks on the box, including the sleeve inside the box. If unauthorized locks are discovered, the bank reserves the right to remove them without notice. The bank is not responsible for any damage to the lock or contents in this situation. The bank also mentions that they are not liable for any delays caused by the failure of the locks on the box to operate. Additionally, renters receive two keys and agree to return them upon surrendering the box or terminating the agreement. The bank may charge a key deposit and reserves the right to drill open the box if needed, with two bank employees present to remove, examine, and inventory the contents.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"What is the locking and unlocking system like at BOFA?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
